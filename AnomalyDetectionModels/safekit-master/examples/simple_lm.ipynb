{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-tier language model example\n",
    "\n",
    "The following tutorial demonstrates how to utilize safekit's language modeling recurrent neural network to perform event-level anomaly detection. Unlike the aggregate autoencoder and its baselines, the language model is capable of detecting anomalous behavior at the event level. It accomplishes this by attempting to learn the syntax of log lines and the semantic relationships between individual fields in a log line. This allows the model to predict not only the likelihood of a network event, but also the likelihood of individual features appearing at given positions in the log line representation of that event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from safekit.batch import OnlineBatcher\n",
    "from safekit.graph_training_utils import ModelRunner, EarlyStop\n",
    "from safekit.tf_ops import lm_rnn\n",
    "from safekit.util import get_mask, Parser\n",
    "\n",
    "tf.set_random_seed(408)\n",
    "np.random.seed(408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define some hyperparameters for our model—these will be explained in greater detail as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = [10]\n",
    "lr = 1e-3\n",
    "embed_size = 20\n",
    "mb_size = 64\n",
    "\n",
    "maxbadcount = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the JSON file describing the specifications for the data.\n",
    "\n",
    "This JSON file describes a dictionary specifying the number of features in the input data; the categories corresponding to the features; whether the corresponding category is metadata, input, or output; and the indices which map these categories to specific features. This dictionary can later be used to ease interaction with the data when providing it as input to Tensorflow.\n",
    "\n",
    "`sentence_length` specifies a fixed sequence length over which our model will perform backpropagation through time, and `token_set_size` specifies the size of the vocabulary comprising all of the sequences—the former will be used to define the shape of the placeholders used for the features and targets, while the latter is used to define the shape of the embedding matrix used to map our categorical features to embedded representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataspecs = json.load(open('../safekit/features/specs/lm/lanl_word_config.json', 'r'))\n",
    "sentence_length = dataspecs['sentence_length'] - 1\n",
    "token_set_size = dataspecs['token_set_size']\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, sentence_length])\n",
    "t = tf.placeholder(tf.int32, [None, sentence_length])\n",
    "ph_dict = {'x': x, 't': t}\n",
    "\n",
    "token_embed = tf.Variable(tf.truncated_normal([token_set_size, embed_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the recurrent neural network proper. A call to `lm_rnn` will instantiate all of the graph operations comprising our RNN and return a tuple of tensors: `token_losses`, which represents the token-wise losses over each input sequence; `h_states`, a sentence-length tensor comprised of the hidden states at each time step; and `final_h`, simply the hidden state at the last time step. For this call, we pass our input and output placeholders as well as our embedding matrix. We also provide a list of hidden layer sizes which determines the dimensionality of the hidden states at each time step—specifying more than one layer size will yield a stacked RNN architecture. The resulting model is a single-tiered RNN using Long Short Term Memory cells with a hidden dimensionality of 10.\n",
    "\n",
    "Finally, we define our losses over individual lines and over all lines by first averaging the feature-wise losses, then averaging these losses over an entire batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wxh/AnomalyDetectionModels/safekit-master/safekit/tf_ops.py:274: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "token_losses, h_states, final_h = lm_rnn(x, t, token_embed, layer_list)\n",
    "\n",
    "line_losses = tf.reduce_mean(token_losses, axis=1)\n",
    "avg_loss = tf.reduce_mean(line_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To map losses back to our input features easily, we'll next define a function that we can call during the training loop that will write metadata and losses for each data point in the current minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('results', 'w')\n",
    "outfile.write(\"batch line second day user red loss\\n\")\n",
    "\n",
    "def write_results(data_dict, loss, outfile, batch):\n",
    "    for n, s, d, u, r, l in zip(data_dict['line'].flatten().tolist(),\n",
    "                                data_dict['second'].flatten().tolist(),\n",
    "                                data_dict['day'].flatten().tolist(),\n",
    "                                data_dict['user'].flatten().tolist(),\n",
    "                                data_dict['red'].flatten().tolist(),\n",
    "                                loss.flatten().tolist()):\n",
    "        outfile.write('%s %s %s %s %s %s %r\\n' % (batch, int(n), int(s), int(d), int(u), int(r), l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a `ModelRunner` object, which provides a simple interface for interacting with the Tensorflow session. Instantiating this object will define the optimizer Tensorflow will use for gradient descent and initialize all of the variables in the Tensorflow graph. We can then use the `train_step` method on this object to perform an optimization step or the `eval` method to retrieve the values of arbitrary tensors in the graph.\n",
    "\n",
    "In order to record the losses for all of the features, we define a list `eval_tensors` that contains tensors whose values we want to retrieve during training. We'll provide this list to the `ModelRunner`'s `eval` method during the training loop to compute these tensors, then record their values with the `write_results` function defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelRunner(avg_loss, ph_dict, learnrate=lr)\n",
    "\n",
    "eval_tensors = [avg_loss, line_losses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our experiments, we want to first train our model on a single day of user activity, evaluate the model's performance on the next day, then repeat this process for each day in the data. To ease this process, we'll define a function that will either train or evaluate our model over a single day of events.\n",
    "\n",
    "We first instantiate a batcher to divide the data into smaller portions. Since each day may contain a large number of events, we want to provide it to the model in small batches to avoid filling memory. Adjusting the minibatch size may also improve the model's performance. Here, we'll use a batch size of 64 data points, defined above as `mb_size`.\n",
    "\n",
    "We then define a stopping criteria for training using the `EarlyStop` object; if our model's performance doesn't improve after 10 training steps—defined above as `maxbadcount`—the `check_error` function we instantiate will return `False`, and training will be discontinued.\n",
    "\n",
    "In order to prepare data for training or evaluation, we manipulate raw batches from our batcher to construct a dictionary for Tensorflow that maps features to the placeholders used to feed data into the computational graph during training. We map the metadata features to their respective dictionary fields, define the upper range of our inputs and outputs with the `endx` and `endt` variables, then use these to select the appropriate features in the raw batch to determine our input and output.\n",
    "\n",
    "During training, we retrieve the losses for the current batch, then perform a training step to perform gradient descent over a single batch of inputs. This process repeats until either the batcher has reached the end of the input file, the stopping criteria has been met, or the model's error has diverged to infinity. During evaluation, we only retrieve the losses, then write these to our results file using `write_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainday(is_training, f):\n",
    "    batch_num = 0\n",
    "    #data = OnlineBatcher('/home/hutch_research/data/lanl/char_feats/word_day_split/' + f, mb_size, delimiter=' ')\n",
    "    data = OnlineBatcher('/home/wxh/AnomalyDetectionModels/safekit-master/data_examples/lanl/lm_feats/word_day_split/' + f, mb_size, delimiter=' ')\n",
    "    raw_batch = data.next_batch()\n",
    "    cur_loss = sys.float_info.max\n",
    "    check_error = EarlyStop(maxbadcount)\n",
    "    endx = raw_batch.shape[1] - 1\n",
    "    endt = raw_batch.shape[1]\n",
    "    training = check_error(raw_batch, cur_loss)\n",
    "    while training:\n",
    "        data_dict = {'line': raw_batch[:, 0], 'second': raw_batch[:, 1], \n",
    "                     'day': raw_batch[:, 2], 'user': raw_batch[:, 3], \n",
    "                     'red': raw_batch[:, 4], 'x': raw_batch[:, 5:endx],\n",
    "                     't': raw_batch[:, 6:endt]}\n",
    "\n",
    "        _, cur_loss, pointloss = model.train_step(data_dict, eval_tensors, update=is_training)\n",
    "        if not is_training:\n",
    "            write_results(data_dict, pointloss, outfile, batch_num)\n",
    "        batch_num += 1\n",
    "        \n",
    "        print('%s %s %s %s %s %s %r' % (raw_batch.shape[0], data_dict['line'][0],\n",
    "                                        data_dict['second'][0], ('fixed', 'update')[is_training],\n",
    "                                        f, data.index, cur_loss))\n",
    "        \n",
    "        raw_batch = data.next_batch()\n",
    "        training = check_error(raw_batch, cur_loss)\n",
    "        if training < 0:\n",
    "            exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For concision, we will train and evaluate our model on a small subset of our data. To train and evaluate over the entire data set, uncomment the lines following the current definition of `files`.\n",
    "\n",
    "Notice that if we use the entire data set, we reference a field in our data specifications called `weekend_days`. In our configuration files, we have specified a list of days in our data set which correspond to weekends. We want to exclude these days from training simply because they represent different patterns of user activity that may not match the distribution of user activities found during weekdays. To include these events in our analyses without affecting accuracy, another model can be trained on these events.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = dataspecs['test_files']\n",
    "\n",
    "# weekend_days = dataspecs['weekend_days']\n",
    "# files = [str(i) + '.txt' for i in range(dataspecs[\"num_days\"]) if i not in weekend_days]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we enter the training loop, which simply consists of two successive calls to `trainday`. The first call trains the model on the current day, and the second call evaluates the model on the following day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datadict: {'second': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'user': array([1.010e+02, 1.010e+02, 1.000e+01, 1.000e+01, 1.137e+03, 1.137e+03,\n",
      "       1.190e+02, 1.190e+02, 1.290e+02, 1.290e+02, 1.470e+02, 1.470e+02,\n",
      "       1.470e+02, 1.470e+02, 1.470e+02, 1.500e+01, 1.500e+01, 1.750e+02,\n",
      "       1.750e+02, 1.750e+02, 1.750e+02, 1.750e+02, 1.750e+02, 1.750e+02,\n",
      "       1.750e+02, 1.750e+02, 1.750e+02, 1.750e+02, 1.750e+02, 1.782e+03,\n",
      "       1.782e+03, 1.782e+03, 1.880e+02, 1.980e+02, 1.980e+02, 1.000e+00,\n",
      "       2.000e+01, 2.000e+01, 2.100e+01, 2.200e+01, 2.200e+01, 2.300e+01,\n",
      "       2.483e+03, 2.500e+01, 2.500e+01, 2.600e+01, 2.600e+01, 2.600e+01,\n",
      "       2.600e+01, 2.600e+01, 2.700e+01, 3.000e+01, 3.000e+01, 3.000e+01,\n",
      "       3.270e+02, 3.200e+01, 3.000e+00, 3.000e+00, 3.000e+00, 3.000e+00,\n",
      "       4.150e+02, 4.150e+02, 4.500e+01, 4.500e+01]), 'x': array([[ 0.,  3.,  4.,  5.,  4.,  5.,  5.,  6.,  7.,  8.,  9.],\n",
      "       [ 0.,  3.,  4.,  3.,  4.,  5.,  5., 10., 11., 12.,  9.],\n",
      "       [ 0., 13.,  4., 13.,  4., 14., 14., 15., 16., 12.,  9.],\n",
      "       [ 0., 13.,  4., 13.,  4., 17., 18., 15., 16., 12.,  9.],\n",
      "       [ 0., 19.,  4., 19.,  4., 20., 20.,  6., 16., 21.,  9.],\n",
      "       [ 0., 19.,  4., 19.,  4., 22., 22.,  6., 16., 21.,  9.],\n",
      "       [ 0., 23.,  4., 23.,  4., 14., 14.,  6., 16., 21.,  9.],\n",
      "       [ 0., 23.,  4., 23.,  4., 14., 14., 15., 16., 12.,  9.],\n",
      "       [ 0., 24.,  4., 24.,  4., 25., 25.,  6., 16., 21.,  9.],\n",
      "       [ 0., 24.,  4., 24.,  4., 25., 25., 15., 16., 12.,  9.],\n",
      "       [ 0., 26.,  4., 18.,  4., 18., 18.,  6.,  7.,  8.,  9.],\n",
      "       [ 0., 26.,  4., 22.,  4., 22., 22.,  6.,  7.,  8.,  9.],\n",
      "       [ 0., 26.,  4., 26.,  4., 27., 27.,  6.,  7., 28.,  9.],\n",
      "       [ 0., 26.,  4., 26.,  4., 27., 18.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 26.,  4., 26.,  4., 27., 22.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 30.,  4., 30.,  4., 25., 25.,  6., 16., 21.,  9.],\n",
      "       [ 0., 30.,  4., 30.,  4., 25., 25., 15., 16., 12.,  9.],\n",
      "       [ 0., 31.,  4., 32.,  4., 32., 32.,  6.,  7.,  8.,  9.],\n",
      "       [ 0., 31.,  4., 33.,  4., 33., 33.,  6.,  7.,  8.,  9.],\n",
      "       [ 0., 31.,  4., 31.,  4., 34., 34.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 31.,  4., 31.,  4., 34., 34.,  6.,  7., 28.,  9.],\n",
      "       [ 0., 31.,  4., 31.,  4., 35., 36.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 31.,  4., 31.,  4., 35., 35.,  6.,  7., 28.,  9.],\n",
      "       [ 0., 31.,  4., 31.,  4., 37., 38.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 31.,  4., 31.,  4., 37., 37.,  6.,  7., 28.,  9.],\n",
      "       [ 0., 31.,  4., 31.,  4., 39., 40.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 31.,  4., 31.,  4., 39., 39.,  6.,  7., 28.,  9.],\n",
      "       [ 0., 31.,  4., 31.,  4., 33., 33.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 31.,  4., 31.,  4., 33., 33.,  6.,  7., 28.,  9.],\n",
      "       [ 0., 41.,  4., 41.,  4., 42., 42.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 41.,  4., 41.,  4., 42., 42.,  6.,  7., 28.,  9.],\n",
      "       [ 0., 41.,  4., 41.,  4., 42., 42., 10., 43., 12., 44.],\n",
      "       [ 0., 45.,  4., 46.,  4., 46., 46.,  6.,  7.,  8.,  9.],\n",
      "       [ 0., 47.,  4., 47.,  4., 48., 48.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 47.,  4., 47.,  4., 48., 48.,  6.,  7., 28.,  9.],\n",
      "       [ 0., 49.,  4., 49.,  4., 50., 51., 15., 16., 12.,  9.],\n",
      "       [ 0., 52.,  4., 52.,  4., 53., 54.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 52.,  4., 52.,  4., 55., 54.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 56.,  4., 57.,  4., 58., 58.,  6.,  7.,  8.,  9.],\n",
      "       [ 0., 59.,  4., 59.,  4., 22., 22.,  6., 16., 21.,  9.],\n",
      "       [ 0., 59.,  4., 59.,  4., 60., 22., 15., 16., 12.,  9.],\n",
      "       [ 0., 61.,  4., 62.,  4., 63., 63.,  6.,  7.,  8.,  9.],\n",
      "       [ 0., 64.,  4., 64.,  4., 65., 65.,  6.,  7., 28.,  9.],\n",
      "       [ 0., 57.,  4., 56.,  4., 58., 58., 15., 16., 12.,  9.],\n",
      "       [ 0., 57., 66., 67., 66., 68., 68., 15., 16., 12.,  9.],\n",
      "       [ 0., 69.,  4., 69.,  4., 70., 70.,  6.,  7., 28.,  9.],\n",
      "       [ 0., 69.,  4., 69.,  4., 70., 71.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 69.,  4., 69.,  4., 70., 72.,  6.,  7., 29., 44.],\n",
      "       [ 0., 69.,  4., 69.,  4., 70., 72.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 69.,  4., 73.,  4., 74., 74., 15., 16., 12.,  9.],\n",
      "       [ 0., 73.,  4., 69.,  4., 74., 74.,  6.,  7.,  8.,  9.],\n",
      "       [ 0., 67.,  4., 67.,  4., 55., 55.,  6., 16., 21.,  9.],\n",
      "       [ 0., 67., 66., 57., 66., 68., 68.,  6.,  7.,  8.,  9.],\n",
      "       [ 0., 67., 66., 67., 66., 68., 68.,  6., 16., 21.,  9.],\n",
      "       [ 0., 75.,  4., 75.,  4., 76., 76.,  6., 16., 21.,  9.],\n",
      "       [ 0., 77.,  4., 77.,  4., 78., 22., 15., 16., 12.,  9.],\n",
      "       [ 0., 79.,  4., 79.,  4., 80., 81., 15., 16., 12.,  9.],\n",
      "       [ 0., 79.,  4., 79.,  4., 25., 25.,  6., 16., 21.,  9.],\n",
      "       [ 0., 79.,  4., 79.,  4., 25., 25., 15., 16., 12.,  9.],\n",
      "       [ 0., 79.,  4., 79.,  4., 82., 18., 15., 16., 12.,  9.],\n",
      "       [ 0., 83.,  4., 83.,  4., 84., 84.,  6.,  7., 29.,  9.],\n",
      "       [ 0., 83.,  4., 83.,  4., 84., 84.,  6.,  7., 28.,  9.],\n",
      "       [ 0., 85., 86., 87.,  4., 87., 87.,  6.,  7.,  8.,  9.],\n",
      "       [ 0., 85., 86., 85., 86., 87., 87.,  6., 43., 21.,  9.]]), 'line': array([111., 112., 113., 114., 115., 116., 117., 118., 119., 120., 121.,\n",
      "       122., 123., 124., 125., 126., 127., 128., 129., 130., 131., 132.,\n",
      "       133., 134., 135., 136., 137., 138., 139., 140., 141., 142., 143.,\n",
      "       144., 145., 146., 147., 148., 149., 150., 151., 152., 153., 154.,\n",
      "       155., 156., 157., 158., 159., 160., 161., 162., 163., 164., 165.,\n",
      "       166., 167., 168., 169., 170., 171., 172., 173., 174.]), 'day': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'red': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 't': array([[ 3.,  4.,  5.,  4.,  5.,  5.,  6.,  7.,  8.,  9.,  1.],\n",
      "       [ 3.,  4.,  3.,  4.,  5.,  5., 10., 11., 12.,  9.,  1.],\n",
      "       [13.,  4., 13.,  4., 14., 14., 15., 16., 12.,  9.,  1.],\n",
      "       [13.,  4., 13.,  4., 17., 18., 15., 16., 12.,  9.,  1.],\n",
      "       [19.,  4., 19.,  4., 20., 20.,  6., 16., 21.,  9.,  1.],\n",
      "       [19.,  4., 19.,  4., 22., 22.,  6., 16., 21.,  9.,  1.],\n",
      "       [23.,  4., 23.,  4., 14., 14.,  6., 16., 21.,  9.,  1.],\n",
      "       [23.,  4., 23.,  4., 14., 14., 15., 16., 12.,  9.,  1.],\n",
      "       [24.,  4., 24.,  4., 25., 25.,  6., 16., 21.,  9.,  1.],\n",
      "       [24.,  4., 24.,  4., 25., 25., 15., 16., 12.,  9.,  1.],\n",
      "       [26.,  4., 18.,  4., 18., 18.,  6.,  7.,  8.,  9.,  1.],\n",
      "       [26.,  4., 22.,  4., 22., 22.,  6.,  7.,  8.,  9.,  1.],\n",
      "       [26.,  4., 26.,  4., 27., 27.,  6.,  7., 28.,  9.,  1.],\n",
      "       [26.,  4., 26.,  4., 27., 18.,  6.,  7., 29.,  9.,  1.],\n",
      "       [26.,  4., 26.,  4., 27., 22.,  6.,  7., 29.,  9.,  1.],\n",
      "       [30.,  4., 30.,  4., 25., 25.,  6., 16., 21.,  9.,  1.],\n",
      "       [30.,  4., 30.,  4., 25., 25., 15., 16., 12.,  9.,  1.],\n",
      "       [31.,  4., 32.,  4., 32., 32.,  6.,  7.,  8.,  9.,  1.],\n",
      "       [31.,  4., 33.,  4., 33., 33.,  6.,  7.,  8.,  9.,  1.],\n",
      "       [31.,  4., 31.,  4., 34., 34.,  6.,  7., 29.,  9.,  1.],\n",
      "       [31.,  4., 31.,  4., 34., 34.,  6.,  7., 28.,  9.,  1.],\n",
      "       [31.,  4., 31.,  4., 35., 36.,  6.,  7., 29.,  9.,  1.],\n",
      "       [31.,  4., 31.,  4., 35., 35.,  6.,  7., 28.,  9.,  1.],\n",
      "       [31.,  4., 31.,  4., 37., 38.,  6.,  7., 29.,  9.,  1.],\n",
      "       [31.,  4., 31.,  4., 37., 37.,  6.,  7., 28.,  9.,  1.],\n",
      "       [31.,  4., 31.,  4., 39., 40.,  6.,  7., 29.,  9.,  1.],\n",
      "       [31.,  4., 31.,  4., 39., 39.,  6.,  7., 28.,  9.,  1.],\n",
      "       [31.,  4., 31.,  4., 33., 33.,  6.,  7., 29.,  9.,  1.],\n",
      "       [31.,  4., 31.,  4., 33., 33.,  6.,  7., 28.,  9.,  1.],\n",
      "       [41.,  4., 41.,  4., 42., 42.,  6.,  7., 29.,  9.,  1.],\n",
      "       [41.,  4., 41.,  4., 42., 42.,  6.,  7., 28.,  9.,  1.],\n",
      "       [41.,  4., 41.,  4., 42., 42., 10., 43., 12., 44.,  1.],\n",
      "       [45.,  4., 46.,  4., 46., 46.,  6.,  7.,  8.,  9.,  1.],\n",
      "       [47.,  4., 47.,  4., 48., 48.,  6.,  7., 29.,  9.,  1.],\n",
      "       [47.,  4., 47.,  4., 48., 48.,  6.,  7., 28.,  9.,  1.],\n",
      "       [49.,  4., 49.,  4., 50., 51., 15., 16., 12.,  9.,  1.],\n",
      "       [52.,  4., 52.,  4., 53., 54.,  6.,  7., 29.,  9.,  1.],\n",
      "       [52.,  4., 52.,  4., 55., 54.,  6.,  7., 29.,  9.,  1.],\n",
      "       [56.,  4., 57.,  4., 58., 58.,  6.,  7.,  8.,  9.,  1.],\n",
      "       [59.,  4., 59.,  4., 22., 22.,  6., 16., 21.,  9.,  1.],\n",
      "       [59.,  4., 59.,  4., 60., 22., 15., 16., 12.,  9.,  1.],\n",
      "       [61.,  4., 62.,  4., 63., 63.,  6.,  7.,  8.,  9.,  1.],\n",
      "       [64.,  4., 64.,  4., 65., 65.,  6.,  7., 28.,  9.,  1.],\n",
      "       [57.,  4., 56.,  4., 58., 58., 15., 16., 12.,  9.,  1.],\n",
      "       [57., 66., 67., 66., 68., 68., 15., 16., 12.,  9.,  1.],\n",
      "       [69.,  4., 69.,  4., 70., 70.,  6.,  7., 28.,  9.,  1.],\n",
      "       [69.,  4., 69.,  4., 70., 71.,  6.,  7., 29.,  9.,  1.],\n",
      "       [69.,  4., 69.,  4., 70., 72.,  6.,  7., 29., 44.,  1.],\n",
      "       [69.,  4., 69.,  4., 70., 72.,  6.,  7., 29.,  9.,  1.],\n",
      "       [69.,  4., 73.,  4., 74., 74., 15., 16., 12.,  9.,  1.],\n",
      "       [73.,  4., 69.,  4., 74., 74.,  6.,  7.,  8.,  9.,  1.],\n",
      "       [67.,  4., 67.,  4., 55., 55.,  6., 16., 21.,  9.,  1.],\n",
      "       [67., 66., 57., 66., 68., 68.,  6.,  7.,  8.,  9.,  1.],\n",
      "       [67., 66., 67., 66., 68., 68.,  6., 16., 21.,  9.,  1.],\n",
      "       [75.,  4., 75.,  4., 76., 76.,  6., 16., 21.,  9.,  1.],\n",
      "       [77.,  4., 77.,  4., 78., 22., 15., 16., 12.,  9.,  1.],\n",
      "       [79.,  4., 79.,  4., 80., 81., 15., 16., 12.,  9.,  1.],\n",
      "       [79.,  4., 79.,  4., 25., 25.,  6., 16., 21.,  9.,  1.],\n",
      "       [79.,  4., 79.,  4., 25., 25., 15., 16., 12.,  9.,  1.],\n",
      "       [79.,  4., 79.,  4., 82., 18., 15., 16., 12.,  9.,  1.],\n",
      "       [83.,  4., 83.,  4., 84., 84.,  6.,  7., 29.,  9.,  1.],\n",
      "       [83.,  4., 83.,  4., 84., 84.,  6.,  7., 28.,  9.,  1.],\n",
      "       [85., 86., 87.,  4., 87., 87.,  6.,  7.,  8.,  9.,  1.],\n",
      "       [85., 86., 85., 86., 87., 87.,  6., 43., 21.,  9.,  1.]])}, shape: ['second', 'user', 'x', 'line', 'day', 'red', 't']\n",
      "('ph_dict: ', {'x': <tf.Tensor 'Placeholder:0' shape=(?, 10) dtype=int32>, 't': <tf.Tensor 'Placeholder_1:0' shape=(?, 10) dtype=int32>})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 11) for Tensor u'Placeholder:0', which has shape '(?, 10)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-aec24870cd56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrainday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtrainday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-53699566938c>\u001b[0m in \u001b[0;36mtrainday\u001b[1;34m(is_training, f)\u001b[0m\n\u001b[0;32m     15\u001b[0m                      't': raw_batch[:, 6:endt]}\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpointloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mwrite_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpointloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/wxh/AnomalyDetectionModels/safekit-master/safekit/graph_training_utils.pyc\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, datadict, eval_tensors, update)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ph_dict: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mph_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         return self.sess.run(train_op + eval_tensors,\n\u001b[1;32m--> 104\u001b[1;33m                              feed_dict=get_feed_dict(datadict, self.ph_dict, debug=self.debug))\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/wxh/.conda/envs/safekti/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/wxh/.conda/envs/safekti/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1128\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1129\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (64, 11) for Tensor u'Placeholder:0', which has shape '(?, 10)'"
     ]
    }
   ],
   "source": [
    "for idx, f in enumerate(files[:-1]):\n",
    "    trainday(True, f)\n",
    "    trainday(False, files[idx + 1])\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a72995a5e612b59b572113865b3eff9d28ba16dd7068b20ae9f4a4b8b526a60"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
